{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "category.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/raymondhs/pytorch-category-suggestion/blob/master/category.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "BcFyR9QpMTiL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "-6J5r8IyL9Y7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ]
    },
    {
      "metadata": {
        "id": "UuD-P6NIDHVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "128153b1-6847-4637-fa07-b6c533cc9398"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchtext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.2.3)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.25.0)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.8.13)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d0ufATsKE7j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d640992b-7091-45ad-fa04-5d6e5f07d19c"
      },
      "cell_type": "code",
      "source": [
        "! [ ! -f pytorch-category-suggestion ] && git clone --recursive https://github.com/raymondhs/pytorch-category-suggestion/\n",
        "! [ ! -f data ] && cp -r pytorch-category-suggestion/data ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-category-suggestion'...\n",
            "remote: Counting objects: 9, done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4KOnco7zLO0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDNcAJ2CMx-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "_PoElGl3M19b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Define Field objects corresponding to Product Title and Category\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XNDeoI2ADHVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TITLE = data.Field()\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvNetHS7M-RP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Load datasets"
      ]
    },
    {
      "metadata": {
        "id": "eNgNq6FIM9ew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, test = data.TabularDataset.splits(\n",
        "    path='data/category', format='csv',\n",
        "    train='train.csv', validation='test.csv',\n",
        "    fields=[('title', TITLE), ('label', LABEL)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "th-59zASPnS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCemUkPhNDXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Build vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "CSYz1uvkNGse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TITLE.build_vocab(train)\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FgWLGXVPNI81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Create dataset iterator"
      ]
    },
    {
      "metadata": {
        "id": "TZwICcmbNLkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = data.Iterator.splits(\n",
        "    (train, test), batch_sizes=(1, 1),\n",
        "    repeat=False, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUSy5tnSNP81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FastText\n",
        "\n",
        "Let's implement FastText in PyTorch.\n",
        "\n",
        "Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.\n",
        "\n",
        "![fasttext](https://github.com/raymondhs/pytorch-category-suggestion/raw/master/fig/fasttext.png)"
      ]
    },
    {
      "metadata": {
        "id": "oL9awkGpDHVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FastTextClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
        "        # Calls the init function of nn.Module.\n",
        "        super().__init__()\n",
        "        \n",
        "        # Stores a lookup table for each word\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # Creates a linear projection (y = Ax + b)\n",
        "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "        # LogSoftmax layer: Transform scores to log-probabilities\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Shape of x is (num_words, batch_size)\n",
        "        # .t() will transpose x to (batch_size, num_words)\n",
        "        # Output of nn.Embedding is\n",
        "        # (batch_size, num_words, embedding_dim)\n",
        "        embedded = self.embedding(x.t())\n",
        "        \n",
        "        # Average the word vectors!\n",
        "        # dim=1 corresponds to word position\n",
        "        mean_vec = torch.mean(embedded, dim=1)\n",
        "        \n",
        "        # Pass the input through the linear layer,\n",
        "        # then pass that through log_softmax.\n",
        "        return self.log_softmax(self.linear(mean_vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DEDnBuYkPq5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "f4wELzGsPyDD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Create an instance of our FastText module"
      ]
    },
    {
      "metadata": {
        "id": "UdYbenhADHVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "800ed4c8-9612-4f6e-ac7e-f37075ebe5c5"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(TITLE.vocab)\n",
        "# This is a hyperparameter:\n",
        "# The size of the word vectors\n",
        "hidden_size = 20\n",
        "num_labels = len(LABEL.vocab)\n",
        "\n",
        "model = FastTextClassifier(vocab_size,\n",
        "                           hidden_size,\n",
        "                           num_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 = 449.0738\n",
            "loss at epoch 10 = 31.2956\n",
            "loss at epoch 20 = 5.1067\n",
            "loss at epoch 30 = 2.2203\n",
            "loss at epoch 40 = 1.3538\n",
            "loss at epoch 50 = 0.9571\n",
            "loss at epoch 60 = 0.7316\n",
            "loss at epoch 70 = 0.5880\n",
            "loss at epoch 80 = 0.4891\n",
            "loss at epoch 90 = 0.4173\n",
            "final loss = 0.3677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hcnXVEbDP6F2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Define a training objective:\n",
        "  * Minimize a loss function called the negative log-likelihood (NLL)\n"
      ]
    },
    {
      "metadata": {
        "id": "RoIk43ftQHC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2q3SUfZmQH6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Define an optimizer:\n",
        "  * Stochastic gradient descent (SGD)\n",
        "  * Hyperparameter: Learning Rate"
      ]
    },
    {
      "metadata": {
        "id": "9v3a4nk4QQiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_bF9vXJQTrE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Let's train for 100 epochs!"
      ]
    },
    {
      "metadata": {
        "id": "sRbMimPKP1RV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "\n",
        "# Train for 100 epochs\n",
        "# 1 epoch = 1 full pass of training data\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for instance in train_iter:\n",
        "        # Step 1. Remember that PyTorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Run our forward pass.\n",
        "        log_probs = model(instance.title)\n",
        "\n",
        "        # Step 3. Compute the loss, gradients, and\n",
        "        # update the parameters by calling optimizer.step()\n",
        "        loss = loss_function(log_probs, instance.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(\"loss at epoch {} = {:.4f}\".format(epoch, total_loss))\n",
        "\n",
        "print(\"final loss = {:.4f}\".format(total_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VGlDZW3LQbVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ]
    },
    {
      "metadata": {
        "id": "slQUrZhwDHV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e32d4352-c1cc-4a07-8d42-e957b656b091"
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    num_correct = 0\n",
        "    for instance in test_iter:\n",
        "        log_probs = model(instance.title)\n",
        "        prediction = torch.argmax(log_probs, dim=1)\n",
        "        if (prediction == instance.label):\n",
        "            num_correct += 1\n",
        "    accuracy = num_correct*100/len(test)\n",
        "    print(\"Acc.: {:.2f}%\".format(accuracy))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc.: 96.33%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}